---
title: 'Data Science Capstone - Week 3 - Prediction Model'
author: "Crisitan Neuhaus"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, include=FALSE}
if(!require(AppliedPredictiveModeling))install.packages("AppliedPredictiveModeling")
if(!require(caret))install.packages("caret")
if(!require(pgmm))install.packages("pgmm")
if(!require(rpart))install.packages("rpart")
if(!require(tidyverse))install.packages("tidyverse")
if(!require(randomForest))install.packages("randomForest")
if(!require(rpart.plot))install.packages("rpart.plot")
if(!require(corrplot))install.packages("corrplot")
if(!require(rattle))install.packages("rattle")
if(!require(tm))install.packages("tm")
if(!require(downloader))install.packages("downloader")
if(!require(stringi))install.packages("stringi")
if(!require(wordcloud))install.packages("wordcloud")
if(!require(plotly))install.packages("plotly")
if(!require(RWeka))install.packages("RWeka")
```
Libraries used:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(AppliedPredictiveModeling)
library(caret)
library(pgmm)
library(rpart)
library(tidyverse)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(rattle)
library(tm)
library(downloader)
library(stringi)
library(wordcloud)
library(plotly)
library(RWeka)
```
Task 4 - Prediction Model  
The goal of this exercise is to build and evaluate your first predictive model. You will use the n-gram and backoff models you built in previous tasks to build and evaluate your predictive model. The goal is to make the model efficient and accurate. 

Tasks to accomplish  

Build a predictive model based on the previous data modeling steps - you may combine the models in any way you think is appropriate.  
Evaluate the model for efficiency and accuracy - use timing software to evaluate the computational complexity of your model. Evaluate the model accuracy using different metrics like perplexity, accuracy at the first word, second word, and third word.  

Questions to consider  

How does the model perform for different choices of the parameters and size of the model?  
How much does the model slow down for the performance you gain?  
Does perplexity correlate with the other measures of accuracy?  
Can you reduce the size of the model (number of parameters) without reducing performance?  

## Peer-graded Assignment Overview

**Instructions**  
The goal of this project is to provide an exploratory text data analysis on the data that we are on track to create a prediction algorithm. This document shows and explain only the major features of the data and a briefly summary.  
The main plan is to provide an understandable view of data to create, in the next steps, a Shiny app in a way that it predict the next word using **natural language processing**. This summary contains tables and plots to illustrate the data set. 

**Dataset**  
This dataset is fairly large and is from [Capstone data Set](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip), Coursera Data Science Capstone class and contain English, German, Russian and Finnish database. But we will focus on English database.  
Each database contains three files with text corpus of documents to be used as training data:  
 - *en_US.blogs.txt*  
 - *en_US.news.txt*  
 - *en_US.twiter.txt*  

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Junk to download file and unzip it
Url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists('final')) {
    dir.create('final')
}
if (!file.exists("final/en_US")) {
    if (Sys.info()[['sysname']] == 'Windows'){
      download.file(Url,destfile="Coursera-SwiftKey.zip") # for window
      unzip ("Coursera-SwiftKey.zip")
    }
    if (Sys.info()[['sysname']] == 'Darwin'){
      download.file(Url,destfile="Coursera-SwiftKey.zip",method="curl") # for mac
      unzip ("Coursera-SwiftKey.zip", exdir = "./")
    }
}
```
## Exploratory data analysis  

## Size of files  
To build our model, we don't need to load in and use all of the data due to the large of data.  

```{r echo=FALSE, warning=FALSE}
if(FALSE){ # it takes a long time processing and will not be used
files_data <- c("./final/en_US/en_US.twitter.txt",
                   "./final/en_US/en_US.blogs.txt",
                   "./final/en_US/en_US.news.txt")
total_ln <- c();file_size <- c();total_word <- c()
for(i in 1:length(files_data)){
  file_size[i] <- as.integer(file.size(paste(files_data[i],sep = '"'))/1024/1024) # Megabyte
  con <- file(paste(files_data[i],sep = '"'), "r")  # Open connection
  temp <- readLines(con) # read lines to temp
  close(con)
  total_ln[i] <- as.integer(length(temp)) # Total lines
  total_word[i] <- sum(sapply(strsplit(temp, " "), length)) # Total Words
}
print(data.frame(
                  file = files_data,
                  total_lines = total_ln,
                  total_words = total_word,
                  size_mb = file_size))
}
```

```{r message=FALSE}
# Files path and names
files_data <- c("./final/en_US/en_US.twitter.txt",
                   "./final/en_US/en_US.blogs.txt",
                   "./final/en_US/en_US.news.txt")

# Read all three data sets
twitter<-readLines(files_data[1],warn=FALSE,encoding="UTF-8")
blogs<-readLines(files_data[2],warn=FALSE,encoding="UTF-8")
news<-readLines(files_data[3],warn=FALSE,encoding="UTF-8")

# Get files details
files_data_info <- data.frame(
  file = files_data, 
  size_mb = round(file.info(files_data)$size/(1024^2), digits = 2))

# Add new 3 columns
files_data_info$total_lines <- 0
files_data_info$total_words <- 0
files_data_info$total_char <- 0

# Get #rows
files_data_info[1,]$total_lines <- length(twitter)
files_data_info[2,]$total_lines <- length(blogs)
files_data_info[3,]$total_lines <- length(news)

# Get #characters
files_data_info[1,]$total_char <- sum(nchar(twitter))
files_data_info[2,]$total_char <- sum(nchar(blogs))
files_data_info[3,]$total_char <- sum(nchar(news))

# Get #words
files_data_info[1,]$total_words <- stri_stats_latex(twitter)[4]
files_data_info[2,]$total_words <- stri_stats_latex(blogs)[4]
files_data_info[3,]$total_words <- stri_stats_latex(news)[4]

files_data_info
```


## Sampling  

The plan will get a representative sample of data by reading in a random subset of the original data.  
Bellow are some details from the result file.  

```{r message=FALSE}
# set seed for reproducibility
set.seed(3433)

# Sampling the data to 0.01 - for better performance
twitter <- sample(twitter, length(twitter) * 0.1, replace = FALSE)
blogs <- sample(blogs, length(blogs) * 0.1, replace = FALSE)
news <- sample(news, length(news) * 0.1, replace = FALSE)

# combine all three data sets into a single data set
inTrain <- c(twitter, blogs, news)
rm(twitter, blogs, news) # remove data not needed
```

## Cleanning Data & Text Normalization  

The cleaning consists to transform the following topics:

* *Remove non-English characters*
* *Convert all words to lowercase*
* *Removing punctuation, number, special characters, white-spaces, etc.*
* *Removing stop words*
* *Stemming the text*
* *Remove URLs*

Many of these steps are performed using  [***tm***](https://cran.r-project.org/web/packages/tm/index.html) package that provides a comprehensive text mining framework for R. Also the [Lecture Slides from the 2012 Stanford Coursera course](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) provide a good references to have a better understanding.

The *tm* package uses the concept of a so-called source to encapsulate and abstract the document input process. In this way a corpus is created from the text already sampled, using the *tm* pacakge, in order to take advantage of the text mining functionalities provided by that package.

```{r message=FALSE}
buildCorpus <- function (dataSet) {
    dataSet <- iconv(dataSet, "latin1", "ASCII", sub = "") # remove non-English words
    docs <- VCorpus(VectorSource(dataSet))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
    # remove URL, Twitter handles and email patterns, transform to lower case etc.
    docs <- tm_map(docs, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
    docs <- tm_map(docs, toSpace, "@[^\\s]+")
    docs <- tm_map(docs, toSpace, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b")
    docs <- tm_map(docs, tolower)
    docs <- tm_map(docs, removeWords, stopwords("english"))
    docs <- tm_map(docs, removePunctuation)
    docs <- tm_map(docs, removeNumbers)
    docs <- tm_map(docs, stripWhitespace)
    docs <- tm_map(docs, PlainTextDocument)
    return(docs)
}
corpus <- buildCorpus(inTrain) # function call
```

## Exploratory Data Analysis

Exploratory data is to provide to you a basic summary about the data set. Word counts, showing more frequent words, frequencies of 2-grams and 3-grams, line counts, a basic data table and a basic plots to show you some features of the data.  
As a result file, that combined data from *twitter*, *blogs* and *news*, where was considered 1% as sample size for both files. we have the resume as:    

```{r}
message(sprintf(" %s lines\n %s words\n %s characters\n %s MB", length(inTrain), stri_stats_latex(inTrain)[4], sum(nchar(inTrain)), round(object.size(inTrain)/1024^2, digits = 2)))
#rm(inTrain) # remove data not needed
```

### Word Frequencies

A bar chart and word cloud are showed to illustrate most common word that appears into the result file.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
tdm <- TermDocumentMatrix(corpus)
freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
wordFreq <- data.frame(word = names(freq), freq = freq)

# plot the top 100 most frequent words
wordFreq %>% 
  arrange(desc(freq)) %>%
  top_n(100, freq) %>% 
  mutate(
      word = fct_reorder(word, freq, .desc = TRUE)
  ) %>% 
  plot_ly(x = ~word, y = ~freq, type = "bar") %>% 
  layout(title = 'Top 100 most frequent words')

# construct word cloud using library(wordcloud)
suppressWarnings (
    wordcloud(words = wordFreq$word,
              freq = wordFreq$freq,
              min.freq = 1,
              max.words = 100,
              random.order = FALSE,
              rot.per = 0.35, 
              colors=brewer.pal(8, "Dark2"))
)

# remove variables no longer needed to free up memory
rm(tdm, freq, wordFreq)
```


### Tokenizing and N-Gram Generation

To provide a frequency that words appears in pairs or triplets together, We use the `RWeka` package
to build functions to tokenize the sample data and provide matrices of uniqrams, bigrams, and trigrams.  
According to [Wikipedia](https://en.wikipedia.org/wiki/N-gram), Using Latin numerical prefixes, an n-gram of size 1 is referred to as a "unigram"; size 2 is a "bigram" (or, less commonly, a "digram"); size 3 is a "trigram"...  

#### Unigrams  

```{r, message = FALSE, echo = FALSE}
# Function for unigrams
unigramToken <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))

# create term document matrix for the corpus
unigramMatrix <- TermDocumentMatrix(corpus, control = list(tokenize = unigramToken))

# eliminate sparse terms for each n-gram and get frequencies of most common n-grams
unigramMatrixFreq <- sort(rowSums(as.matrix(removeSparseTerms(unigramMatrix, 0.99))), decreasing = TRUE)
unigramMatrixFreq <- data.frame(word = names(unigramMatrixFreq), freq = unigramMatrixFreq)

# generate plot
unigramMatrixFreq %>% 
  arrange(desc(freq)) %>%
  top_n(20, freq) %>% 
  mutate(
      word = fct_reorder(word, freq, .desc = TRUE)
  ) %>% 
  plot_ly(x = ~word, y = ~freq, type = "bar") %>% 
  layout(xaxis = list(title = 'Word'), yaxis = list(title = 'Frequency'),
         title = 'Top 20 most frequent words - Unigrams')
```

#### Bigrams  

```{r, message = FALSE, echo = FALSE}
# Function for bigrams
bigramToken <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

# create term document matrix for the corpus
bigramMatrix <- TermDocumentMatrix(corpus, control = list(tokenize = bigramToken))

# eliminate sparse terms for each n-gram and get frequencies of most common n-grams
bigramMatrixFreq <- sort(rowSums(as.matrix(removeSparseTerms(bigramMatrix, 0.999))), decreasing = TRUE)
bigramMatrixFreq <- data.frame(word = names(bigramMatrixFreq), freq = bigramMatrixFreq)

# generate plot
bigramMatrixFreq %>% 
  arrange(desc(freq)) %>%
  top_n(20, freq) %>% 
  mutate(
      word = fct_reorder(word, freq, .desc = TRUE)
  ) %>% 
  plot_ly(x = ~word, y = ~freq, type = "bar") %>% 
  layout(xaxis = list(title = 'Word'), yaxis = list(title = 'Frequency'),
         title = 'Top 20 most frequent Bigrams')
```

#### Trigrams

```{r, message = FALSE, echo = FALSE}
# Function for trigrams
trigramToken <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

# create term document matrix for the corpus
trigramMatrix <- TermDocumentMatrix(corpus, control = list(tokenize = trigramToken))

# eliminate sparse terms for each n-gram and get frequencies of most common n-grams
trigramMatrixFreq <- sort(rowSums(as.matrix(removeSparseTerms(trigramMatrix, 0.9999))), decreasing = TRUE)
trigramMatrixFreq <- data.frame(word = names(trigramMatrixFreq), freq = trigramMatrixFreq)

# generate plot
trigramMatrixFreq %>% 
  arrange(desc(freq)) %>%
  top_n(20, freq) %>% 
  mutate(
      word = fct_reorder(word, freq, .desc = TRUE)
  ) %>% 
  plot_ly(x = ~word, y = ~freq, type = "bar") %>% 
  layout(xaxis = list(title = 'Word'), yaxis = list(title = 'Frequency'),
         title = 'Top 20 most frequent Trigrams')
```

**Review criteria**  
This Peer Assignment contains some requirements:  
 - Does the link lead to an HTML page describing the exploratory analysis of the training data set?  
 - Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?  
 - Has the data scientist made basic plots, such as histograms to illustrate features of the data?  
 - Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate? 
 
 
## Next Steps  

As mentioned earlier, the next steps will be to produce a Shiny app in a way that will predict the next word using **natural language processing**.  
This first step provided an overview over the data and confirmed the needed to work with a sample size of data due to the large data available.  
The complete code are available into [github.com/Cristianneuhaus](https://github.com/Cristianneuhaus/cristianneuhaus.github.io/tree/master/Coursera/Capstone), PGA-Milestone Report.RMD file. 